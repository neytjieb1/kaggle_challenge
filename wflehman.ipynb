{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from scipy import sparse\n",
    "import scipy.sparse as ss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 2473.35it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('Data/test_data.pkl', 'rb') as file:\n",
    "    test_graphs = pkl.load(file)\n",
    "\n",
    "with open('Data/training_data.pkl', 'rb') as file:\n",
    "    train_graphs = pkl.load(file)\n",
    "    \n",
    "def translateGraphs(GRAPHS):\n",
    "    graphs = []\n",
    "    traduction={0: 50, 1: 51, 2:52, 3:53}\n",
    "    for i in tqdm(range(len(GRAPHS))):\n",
    "        H = GRAPHS[i].copy()\n",
    "        oldlbls = nx.get_edge_attributes(H, 'labels')\n",
    "        newlbls = {e: [traduction[l[0]]] for e, l in oldlbls.items()}\n",
    "        #set edge labels\n",
    "        nx.set_edge_attributes(H, newlbls, name='labels')\n",
    "        #overwrite\n",
    "        graphs.append(H)\n",
    "    return graphs \n",
    "\n",
    "test_graphs = translateGraphs(test_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import getnodelblarr, getedgelabelarr, hashtodic, assignewlabels_node, assignewlabels_edge\n",
    "\n",
    " def WL(graphs, h=4):\n",
    "    #Number of hops\n",
    "    h = 4\n",
    "    N = len(graphs)# N = len(train_graphs)//1\n",
    "    \n",
    "    hashedgraphs = graphs#train_graphs.copy()\n",
    "    hashedgraphs = hashedgraphs[0:N]\n",
    "    Alphabet = {}#{f'{i}': i for i in range(49)} #{}\n",
    "    currentmax = 53\n",
    "\n",
    "    #Alphabet instances for each graph over all hops\n",
    "    #Initialise lv - our list of node-labels\n",
    "    #       and ev - our list of edge-labels\n",
    "    lv = [[] for _ in range(len(hashedgraphs))]\n",
    "    el = [[] for _ in range(len(hashedgraphs))]\n",
    "    for i in range(len(hashedgraphs)):\n",
    "        #get node labels, edgelabels\n",
    "        v = nx.get_node_attributes(hashedgraphs[i], 'labels').values()\n",
    "        e = nx.get_edge_attributes(hashedgraphs[i], 'labels').values()\n",
    "        #extract from list-of-list\n",
    "        v = list(chain.from_iterable(v))\n",
    "        e = list(chain.from_iterable(e))\n",
    "        #add []+a\n",
    "        lv[i] = lv[i] + v\n",
    "        el[i] = el[i] + e\n",
    "\n",
    "    print(f'Hops = {h}')\n",
    "    #For each hop\n",
    "    for _ in range(h):\n",
    "        #For each graph\n",
    "        for i in tqdm(range(len(hashedgraphs))):\n",
    "            #graph to be worked with\n",
    "            Gi = hashedgraphs[i].copy()\n",
    "            #simplify: get biggest connected subgraph\n",
    "            Gi = Gi.subgraph(sorted(nx.connected_components(Gi), key=len, reverse=True)[0])\n",
    "            \n",
    "            #For each node\n",
    "            #Array with columns: node, label, newlabel\n",
    "            lblarri_v = getnodelblarr(Gi)\n",
    "            lblarri_e = getedgelabelarr(Gi)\n",
    "            #Update big alphabet, hash\n",
    "            Alphabet, lblarri_v, currentmax = hashtodic(Alphabet, lblarri_v, currentmax, node=True)\n",
    "            #sort and reset index\n",
    "            lblarri_v = lblarri_v.sort_values(by='node').reset_index(drop=True)  \n",
    "            #relabel to a different graph\n",
    "            # print(lblarri_v, lblarri_e, sep='\\n\\n')\n",
    "            Gi = assignewlabels_node(Gi, lblarri_v)\n",
    "\n",
    "            if len(lblarri_e)!=0:\n",
    "                Alphabet, lblarri_e, currentmax = hashtodic(Alphabet, lblarri_e, currentmax, node=False)\n",
    "                lblarri_e = lblarri_e.sort_values(by='edge').reset_index(drop=True)\n",
    "                Gi = assignewlabels_edge(Gi, lblarri_e)\n",
    "                e = nx.get_edge_attributes(hashedgraphs[i], 'labels').values()\n",
    "                e = list(chain.from_iterable(e))\n",
    "                el[i] = el[i] + e\n",
    "\n",
    "            #assign graph-value\n",
    "            hashedgraphs[i] = Gi\n",
    "            #add counts of labels, reuse previous variable\n",
    "            v = nx.get_node_attributes(hashedgraphs[i], 'labels').values()\n",
    "            v = list(chain.from_iterable(v))\n",
    "            lv[i] = lv[i] + v\n",
    "            \n",
    "    return hashedgraphs, Alphabet, lv, el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hops = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:27<00:00, 292.37it/s]\n",
      "100%|██████████| 8000/8000 [00:34<00:00, 230.96it/s]\n",
      "100%|██████████| 8000/8000 [01:16<00:00, 104.87it/s]\n",
      "100%|██████████| 8000/8000 [02:19<00:00, 57.38it/s]\n"
     ]
    }
   ],
   "source": [
    "hashedgraphs, Alphabet, lv, ev = WL(train_graphs + test_graphs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:02<00:00, 3056.18it/s]\n"
     ]
    }
   ],
   "source": [
    "def generateFeatureVectors(hashedgraphs, Alphabet, l):\n",
    "    '''Creating matrix of feature vectors for each graph'''\n",
    "    M = np.zeros((len(hashedgraphs), len(Alphabet)+100))\n",
    "    for i in tqdm(range(len(hashedgraphs))):\n",
    "        a = pd.value_counts(l[i])\n",
    "        M[i, list(a.index)] = a.values\n",
    "\n",
    "    sM = sparse.csr_matrix(M)\n",
    "    #And save so this only needs to be saved once.\n",
    "    return sM\n",
    "\n",
    "sM = generateFeatureVectors(hashedgraphs, Alphabet, lv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Data for easier reloading later in sparse format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import save_sparse_csr, load_sparse_csr\n",
    "save_sparse_csr('Data/WL_allfeatures_e.npz', sM)\n",
    "save_sparse_csr('Data/WL_trainfeatvec_e.npz', sM[0:len(train_graphs)])\n",
    "save_sparse_csr('Data/WL_testfeatvec_e.npz', sM[len(train_graphs):len(train_graphs+test_graphs)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create kernel matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sM.shape = (8000, 90510)\n",
      "Kernel Matrix Determined\n"
     ]
    }
   ],
   "source": [
    "def kernelFromFeatureVectors(filename, sparse=True):\n",
    "    sM = load_sparse_csr(filename) if sparse else np.loadtxt(filename) #'Data/WLfeaturevectors.npz'\n",
    "    print(f'sM.shape = {sM.shape}')\n",
    "    M = ss.csr_matrix.toarray(sM)\n",
    "    K = np.dot(M, M.T)\n",
    "    sK = ss.csr_matrix(K)\n",
    "    return sK, M\n",
    "\n",
    "sK, M = kernelFromFeatureVectors('Data/WL_allfeatures_e.npz', sparse=True)   \n",
    "print('Kernel Matrix Determined')\n",
    "save_sparse_csr('Data/WLKernel_traintest_e', sK)\n",
    "\n",
    "# Compare how much sparse arrays save space\n",
    "# from sys import getsizeof\n",
    "# M = ss.csr_matrix.toarray(sM)\n",
    "# print(M.shape, getsizeof(M), getsizeof(sM), sep='\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "with open('Data/training_labels.pkl', 'rb') as file:\n",
    "    labels = pkl.load(file)\n",
    "from lib import load_sparse_csr\n",
    "# WLData = load_sparse_csr('Data/WLKernel_traintest_e.npz')\n",
    "WLData = load_sparse_csr('Data/WL\\WL_allfeatures_e_h8.npz.npz')\n",
    "WLData = ss.csr_matrix.toarray(WLData)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project entire dataset (train-validate-test) to PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "evals, evecs = np.linalg.eigh(WLData)\n",
    "numpca = 8000\n",
    "WLDataNew = WLData.dot(evecs[0:numpca].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "WLDataNew = ss.fit_transform(WLDataNew)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "WLTrainValid = WLDataNew[0:len(train_graphs)] #, 0:len(train_graphs)]\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(WLTrainValid, labels, test_size=0.3, random_state=1)\n",
    "X_test = WLDataNew[len(train_graphs): len(train_graphs+test_graphs)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Linear Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0, solver=&#x27;newton-cholesky&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0, solver=&#x27;newton-cholesky&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0, solver='newton-cholesky')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "lr = LogisticRegression(random_state=0, solver='newton-cholesky', verbose=False)\n",
    "# Fit on training data\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7563435505993202\n"
     ]
    }
   ],
   "source": [
    "#Predict on validation data\n",
    "pred = lr.predict(X_validate)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_validate, pred, pos_label=1)\n",
    "#AUC metric\n",
    "print(f'AUC: {metrics.auc(fpr, tpr)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 8000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range:\n",
      " [-290.3857713260036, 36.04365338911715]\n"
     ]
    }
   ],
   "source": [
    "from lib import calculateLogits\n",
    "pred_test = lr.predict_proba(X_test)\n",
    "logit = calculateLogits(pred_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to relevant file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import saveDataToFormattedSubmissionFile\n",
    "ctr = int(input('Current submission attempt:\\t'))\n",
    "saveDataToFormattedSubmissionFile(logit, f'Data/WL_test_pred{ctr}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0392687b3784acca957340e081afceb5f7dd41424de88c7591eaa73ceb6efaf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
