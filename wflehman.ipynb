{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_data.pkl', 'rb') as file:\n",
    "    train_graphs = pkl.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SortLabels(GF, nodeindex, labeldict):\n",
    "    '''Get all neighbours of a node at nodeindex and sort them'''\n",
    "    neigh = list(GF.neighbors(nodeindex))\n",
    "    label = [labeldict[i] for i in neigh]\n",
    "    label = list(chain.from_iterable(label))\n",
    "    label.sort()\n",
    "    return label\n",
    "\n",
    "def GlueLabels(nodelabeldict, index, newlist):\n",
    "    '''Glue together a list of nodelabels with the node's own label'''\n",
    "    nodelbl = nodelabeldict[index][0]\n",
    "    x = ''.join([str(s) for s in newlist])\n",
    "    return str(nodelbl) + x\n",
    "\n",
    "def dictToArray(dict):\n",
    "    '''Helper function'''\n",
    "    a = np.array(list(dict.items()), dtype=object )\n",
    "    a[:,1] = list(chain.from_iterable(a[:,1]))\n",
    "    return a\n",
    "\n",
    "def draw(G):\n",
    "    '''Draw graph with its labels'''\n",
    "    pos = nx.spring_layout(G, seed=1, k=0.3)\n",
    "    nx.draw(G, pos)\n",
    "    node_labels = nx.get_node_attributes(G, \"labels\")\n",
    "    nx.draw_networkx_labels(G, pos, node_labels, font_size=8);\n",
    "\n",
    "def getnodelblarr(GF):\n",
    "    '''For a specific graph, generate a (sorted) dataframe of old labels and new glued labels for each node'''\n",
    "    #Dictionary of labels for graph\n",
    "    NLbl_dict = nx.get_node_attributes(GF, 'labels')\n",
    "    #Generate new lbls for each vertex\n",
    "    newlabels = []\n",
    "    for v in NLbl_dict:\n",
    "        sortedlbls = SortLabels(GF, v, NLbl_dict)\n",
    "        gluedlbl = GlueLabels(NLbl_dict, v, sortedlbls)\n",
    "        newlabels.append(gluedlbl)\n",
    "\n",
    "    #Array with columns: node, label, newlabel\n",
    "    Nlbl_arra0 = np.c_[dictToArray(NLbl_dict), newlabels]\n",
    "    #And sort\n",
    "    Nlbl_arra0 = Nlbl_arra0[Nlbl_arra0[:,2].argsort()]\n",
    "    return Nlbl_arra0\n",
    "\n",
    "def hashtodic(ALPHAbet, newlblarr, currentmax):\n",
    "    '''Function to hash newly glued labels; then add the (unique) new ones to the overall alphabet.\n",
    "    Return a Dataframe with new, old, hashed labels for each node'''\n",
    "    #Get unique entries\n",
    "    a = np.unique(newlblarr[:,2]) \n",
    "    #Get those that are new also\n",
    "    a = [a[i] for i in range(len(a)) if a[i] not in ALPHAbet]\n",
    "    #Hash values\n",
    "    b = np.arange(len(a))+currentmax\n",
    "    currentmax = currentmax+len(b)\n",
    "    dic1 = ALPHAbet | {a[i] : b[i] for i in range(len(a))}\n",
    "    #relabel\n",
    "    newlblarr = np.c_[newlblarr, [dic1[newlblarr[:,2][_]] for _ in range(len(newlblarr))]]    \n",
    "    \n",
    "    return dic1, pd.DataFrame(newlblarr, columns=[\"node\", 'oldlbl', 'hashed', 'newlbl']), currentmax\n",
    " \n",
    "def assignewlabels(GF, nlblarr):\n",
    "    '''Helper function: change a graph's labels. (Change directly as working on a copy) '''\n",
    "    newdict = {nlblarr['node'][i] : [nlblarr['newlbl'][i]] for i in range(len(nlblarr))}\n",
    "    H = GF.copy()\n",
    "    nx.set_node_attributes(H, newdict, name='labels')\n",
    "    return H\n",
    "\n",
    "\n",
    "def save_sparse_csr(filename, array):\n",
    "    np.savez(filename, data=array.data, indices=array.indices,\n",
    "             indptr=array.indptr, shape=array.shape)\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return sparse.csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of hops\n",
    "h = 4\n",
    "hashedgraphs = train_graphs.copy()\n",
    "hashedgraphs = hashedgraphs[0:len(train_graphs)//1]\n",
    "Alphabet = {}#{f'{i}': i for i in range(49)} #{}\n",
    "currentmax = 49\n",
    "\n",
    "#Alphabet instances for each graph over all hops\n",
    "#List of 6000 empty lists\n",
    "l = [[] for _ in range(len(hashedgraphs))]\n",
    "for i in range(len(hashedgraphs)):\n",
    "    a = nx.get_node_attributes(hashedgraphs[i], 'labels').values()\n",
    "    a = list(chain.from_iterable(a))\n",
    "    l[i] = l[i] + a\n",
    "pd.value_counts(l[0])\n",
    "\n",
    "maxes = [currentmax]\n",
    "#For each hop\n",
    "for _ in range(h):\n",
    "    #For each graph\n",
    "    for i in tqdm(range(len(hashedgraphs))):\n",
    "        Gi = hashedgraphs[i].copy()# Gi = train_graphs[i]\n",
    "        #get biggest connected subgraph\n",
    "        Gi = Gi.subgraph(sorted(nx.connected_components(Gi), key=len, reverse=True)[0])\n",
    "        #For each node\n",
    "        #Array with columns: node, label, newlabel\n",
    "        lblarri = getnodelblarr(Gi)\n",
    "        #Update big alphabet, hash\n",
    "        Alphabet, lblarri, currentmax = hashtodic(Alphabet, lblarri, currentmax=currentmax)\n",
    "        #sort and reset index\n",
    "        lblarri = lblarri.sort_values(by='node').reset_index(drop=True)\n",
    "        # lblarri = lblarri\n",
    "        #relabel to a different graph\n",
    "        Gi = assignewlabels(Gi, lblarri)\n",
    "        #assign graph-value\n",
    "        hashedgraphs[i] = Gi\n",
    "        #add counts of labels\n",
    "        a = nx.get_node_attributes(hashedgraphs[i], 'labels').values()\n",
    "        a = list(chain.from_iterable(a))\n",
    "        l[i] = l[i] + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,16))\n",
    "# plt.subplot(2,2,1)\n",
    "# draw(graphzero[0])\n",
    "# plt.subplot(2,2,2)\n",
    "# draw(graphzero[1])\n",
    "# plt.subplot(2,2,3)\n",
    "# draw(graphzero[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating matrix of feature vectors for each graph\n",
    "M = np.zeros((len(hashedgraphs), len(Alphabet)+100))\n",
    "for i in tqdm(range(len(hashedgraphs))):\n",
    "    a = pd.value_counts(l[i])\n",
    "    M[i, list(a.index)] = a.values\n",
    "\n",
    "\n",
    "sM = sparse.csr_matrix(M)\n",
    "#And save so this only needs to be saved once.\n",
    "save_sparse_csr('WLfeaturevectors', sM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "print(M.shape, getsizeof(M), getsizeof(sM), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,16))\n",
    "# plt.subplot(2,2,1)\n",
    "# draw(hashedgraphs[9])\n",
    "# plt.subplot(2,2,2)\n",
    "# draw(hashedgraphs[10])\n",
    "# plt.subplot(2,2,3)\n",
    "# # draw(train_graphs[59])\n",
    "# nx.draw(train_graphs[9], pos, with_labels=True)\n",
    "# plt.subplot(2,2,4)\n",
    "# draw(train_graphs[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create kernel matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 51160)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sM = load_sparse_csr('WLfeaturevectors.npz')\n",
    "sM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as ss\n",
    "M = ss.csr_matrix.toarray(sM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(X, Y):\n",
    "    # X is nxp, Y is mxp\n",
    "    return np.dot(X, np.transpose(Y))\n",
    "\n",
    "K = kernel(M, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sK = ss.csr_matrix(K)\n",
    "save_sparse_csr('WLKernel', sM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0392687b3784acca957340e081afceb5f7dd41424de88c7591eaa73ceb6efaf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
