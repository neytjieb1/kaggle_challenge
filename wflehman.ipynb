{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "import scipy.sparse as ss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Own Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import translateGraphs\n",
    "from lib import save_sparse_csr\n",
    "from lib import getnodelblarr, getedgelabelarr, hashtodic, assignewlabels_node, assignewlabels_edge\n",
    "from lib import generateFeatureVectorsfromAlphabet\n",
    "from lib import my_train_test_split\n",
    "from lib import calculateLogits\n",
    "from lib import saveDataToFormattedSubmissionFile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/test_data.pkl', 'rb') as file:\n",
    "    test_graphs = pkl.load(file)\n",
    "\n",
    "with open('Data/training_data.pkl', 'rb') as file:\n",
    "    train_graphs = pkl.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change labels of test graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relabel test_graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 3097.76it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Relabel test_graphs')\n",
    "test_graphs = translateGraphs(test_graphs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate WL Graph Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WL(graphs, h=4):\n",
    "    #Number of hops\n",
    "    N = len(graphs)# N = len(train_graphs)//1\n",
    "    \n",
    "    hashedgraphs = graphs#train_graphs.copy()\n",
    "    hashedgraphs = hashedgraphs[0:N]\n",
    "    Alphabet = {}#{f'{i}': i for i in range(49)} #{}\n",
    "    currentmax = 53\n",
    "\n",
    "    #Alphabet instances for each graph over all hops\n",
    "    #Initialise lv - our list of node-labels\n",
    "    #       and ev - our list of edge-labels\n",
    "    lv = [[] for _ in range(len(hashedgraphs))]\n",
    "    el = [[] for _ in range(len(hashedgraphs))]\n",
    "    for i in range(len(hashedgraphs)):\n",
    "        #get node labels, edgelabels\n",
    "        v = nx.get_node_attributes(hashedgraphs[i], 'labels').values()\n",
    "        e = nx.get_edge_attributes(hashedgraphs[i], 'labels').values()\n",
    "        #extract from list-of-list\n",
    "        v = list(chain.from_iterable(v))\n",
    "        e = list(chain.from_iterable(e))\n",
    "        #add []+a\n",
    "        lv[i] = lv[i] + v\n",
    "        el[i] = el[i] + e\n",
    "\n",
    "    print(f'Hops = {h}')\n",
    "    #For each hop\n",
    "    for _ in range(h):\n",
    "        #For each graph\n",
    "        for i in tqdm(range(len(hashedgraphs))):\n",
    "            #graph to be worked with\n",
    "            Gi = hashedgraphs[i].copy()\n",
    "            #simplify: get biggest connected subgraph\n",
    "            Gi = Gi.subgraph(sorted(nx.connected_components(Gi), key=len, reverse=True)[0])\n",
    "            \n",
    "            #For each node\n",
    "            #Array with columns: node, label, newlabel\n",
    "            lblarri_v = getnodelblarr(Gi)\n",
    "            lblarri_e = getedgelabelarr(Gi)\n",
    "            #Update big alphabet, hash\n",
    "            Alphabet, lblarri_v, currentmax = hashtodic(Alphabet, lblarri_v, currentmax, node=True)\n",
    "            #sort and reset index\n",
    "            lblarri_v = lblarri_v.sort_values(by='node').reset_index(drop=True)  \n",
    "            #relabel to a different graph\n",
    "            # print(lblarri_v, lblarri_e, sep='\\n\\n')\n",
    "            Gi = assignewlabels_node(Gi, lblarri_v)\n",
    "\n",
    "            if len(lblarri_e)!=0:\n",
    "                Alphabet, lblarri_e, currentmax = hashtodic(Alphabet, lblarri_e, currentmax, node=False)\n",
    "                lblarri_e = lblarri_e.sort_values(by='edge').reset_index(drop=True)\n",
    "                Gi = assignewlabels_edge(Gi, lblarri_e)\n",
    "                e = nx.get_edge_attributes(hashedgraphs[i], 'labels').values()\n",
    "                e = list(chain.from_iterable(e))\n",
    "                el[i] = el[i] + e\n",
    "\n",
    "            #assign graph-value\n",
    "            hashedgraphs[i] = Gi\n",
    "            #add counts of labels, reuse previous variable\n",
    "            v = nx.get_node_attributes(hashedgraphs[i], 'labels').values()\n",
    "            v = list(chain.from_iterable(v))\n",
    "            lv[i] = lv[i] + v\n",
    "            \n",
    "    return hashedgraphs, Alphabet, lv, el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hops = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:27<00:00, 295.96it/s]\n",
      "100%|██████████| 8000/8000 [00:32<00:00, 247.93it/s]\n",
      "100%|██████████| 8000/8000 [01:14<00:00, 107.49it/s]\n",
      "100%|██████████| 8000/8000 [02:29<00:00, 53.49it/s]\n",
      "100%|██████████| 8000/8000 [03:38<00:00, 36.54it/s]\n",
      "100%|██████████| 8000/8000 [04:45<00:00, 28.05it/s]\n",
      "100%|██████████| 8000/8000 [05:42<00:00, 23.33it/s]\n",
      "100%|██████████| 8000/8000 [07:28<00:00, 17.84it/s]\n"
     ]
    }
   ],
   "source": [
    "hashedgraphs, Alphabet, lv, ev = WL(train_graphs + test_graphs, h=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate feature vectors from alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:03<00:00, 2539.89it/s]\n"
     ]
    }
   ],
   "source": [
    "#Matrix of feature vectors, will be very sparse.\n",
    "sM = generateFeatureVectorsfromAlphabet(hashedgraphs, Alphabet, lv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Data for easier reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sparse_csr('Data/WL_allfeatures_e.npz', sM)\n",
    "save_sparse_csr('Data/WL_trainfeatvec_e.npz', sM[0:len(train_graphs)])\n",
    "save_sparse_csr('Data/WL_testfeatvec_e.npz', sM[len(train_graphs):len(train_graphs+test_graphs)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create kernel matrix from feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 8000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = np.dot(sM, sM.T)\n",
    "# np.savetxt('WL_Kernel.txt', K)\n",
    "K.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Center dataset: $K^c=(I-U)K(I-U)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 8000)\n"
     ]
    }
   ],
   "source": [
    "# Calculate\n",
    "N = K.shape[0]\n",
    "K = ss.csr_matrix.toarray(K)\n",
    "U = (1/N) * np.ones((N,N))\n",
    "I = np.eye(N)\n",
    "Kc = (I-U) @ K @ (I-U)\n",
    "# np.savetxt('Data/Kc8000.txt', Kc)\n",
    "\n",
    "# Or load previously generated\n",
    "# Kc = np.loadtxt('Data/Kc8000.txt')\n",
    "print(Kc.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/training_labels.pkl', 'rb') as file:\n",
    "    labels = pkl.load(file)\n",
    "\n",
    "WLData = Kc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project entire dataset (train-validate-test) to PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = np.linalg.eigh(Kc)\n",
    "numpca = 8000\n",
    "WLData_projected = Kc.dot(evecs[0:numpca].T)\n",
    "np.savetxt('Data/Kc8000_pcaed.txt', WLData_projected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WLData_projected = np.loadtxt('Data/Kc8000_pcaed.txt')\n",
    "N = 6000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted the use of the SVM classifier from HW2 but the algorithm would not converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lib import my_train_test_split\n",
    "# WLTrainValid = WLData_projected[0:N]\n",
    "# WLLabels = labels[0:N]\n",
    "\n",
    "# from lib import LIN, KernelSVC\n",
    "# sigma = 1.5\n",
    "# C=100.\n",
    "# kernel = LIN().kernel\n",
    "# linmodel = KernelSVC(C=C, kernel=kernel)\n",
    "\n",
    "# X_train, X_validate, y_train, y_validate = my_train_test_split(WLTrainValid, WLLabels, test_size=0.2, random_state=1)\n",
    "# #Does not converge\n",
    "# linmodel.fit(X_train, y_train)\n",
    "# # linmodel.predict(X_validate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also wrote our own kernel logistic regression as in the class slides, but this gave a different error in the exponents. As this was not fixed in time, I opt to use an sklearn library in the hopes of at least showcasing the effectiveness of the WL-algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose subset\n",
    "rows = N\n",
    "cols = 8000\n",
    "WLTrainValid = WLData_projected[0:rows, 0:cols]\n",
    "WLLabels = labels[0:rows]\n",
    "\n",
    "#Split data\n",
    "X_train, X_validate, y_train, y_validate = my_train_test_split(WLTrainValid, WLLabels, test_size=0.2, random_state=1)\n",
    "\n",
    "#Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='newton-cholesky', verbose=False, penalty='l2', C=200, random_state=1)\n",
    "\n",
    "# Fit on training data\n",
    "lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7451557973851092\n"
     ]
    }
   ],
   "source": [
    "#Predict on validation data\n",
    "pred = lr.predict(X_validate[:, 0:1000])\n",
    "\n",
    "#AUC metric\n",
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_validate, pred, pos_label=1)\n",
    "print(f'AUC: {metrics.auc(fpr, tpr)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit range:\n",
      " [-inf, 36.04365338911715]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\norti\\OneDrive - Université Paris-Dauphine\\Documents\\GitHub\\kaggle_challenge\\kaggle_challenge\\lib.py:156: RuntimeWarning: divide by zero encountered in log\n",
      "  logit = np.log(logproba[:,1]/(1-logproba[:,1]))\n"
     ]
    }
   ],
   "source": [
    "X_test = WLData_projected[6000:8000, 0:cols]\n",
    "pred_test = lr.predict_proba(X_test)\n",
    "logit = calculateLogits(pred_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to relevant format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = 'final'#int(input('Current submission attempt:\\t'))\n",
    "saveDataToFormattedSubmissionFile(logit, f'Data/WL_test_pred{ctr}.csv')\n",
    "\n",
    "filename = 'Data/Models/lr_.sav'\n",
    "pkl.dump(lr, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0392687b3784acca957340e081afceb5f7dd41424de88c7591eaa73ceb6efaf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
