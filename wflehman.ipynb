{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from scipy import sparse\n",
    "import scipy.sparse as ss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/test_data.pkl', 'rb') as file:\n",
    "    test_graphs = pkl.load(file)\n",
    "\n",
    "with open('Data/training_data.pkl', 'rb') as file:\n",
    "    train_graphs = pkl.load(file)\n",
    "    \n",
    "def translateGraphs(GRAPHS):\n",
    "    graphs = []\n",
    "    traduction={0: 50, 1: 51, 2:52, 3:53}\n",
    "    for i in tqdm(range(len(GRAPHS))):\n",
    "        H = GRAPHS[i].copy()\n",
    "        oldlbls = nx.get_edge_attributes(H, 'labels')\n",
    "        newlbls = {e: [traduction[l[0]]] for e, l in oldlbls.items()}\n",
    "        #set edge labels\n",
    "        nx.set_edge_attributes(H, newlbls, name='labels')\n",
    "        #overwrite\n",
    "        graphs.append(H)\n",
    "    return graphs \n",
    "\n",
    "test_graphs = translateGraphs(test_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import getnodelblarr, getedgelabelarr, hashtodic, assignewlabels_node, assignewlabels_edge\n",
    "\n",
    "def WL(graphs, h=4):\n",
    "    #Number of hops\n",
    "    N = len(graphs)# N = len(train_graphs)//1\n",
    "    \n",
    "    hashedgraphs = graphs#train_graphs.copy()\n",
    "    hashedgraphs = hashedgraphs[0:N]\n",
    "    Alphabet = {}#{f'{i}': i for i in range(49)} #{}\n",
    "    currentmax = 53\n",
    "\n",
    "    #Alphabet instances for each graph over all hops\n",
    "    #Initialise lv - our list of node-labels\n",
    "    #       and ev - our list of edge-labels\n",
    "    lv = [[] for _ in range(len(hashedgraphs))]\n",
    "    el = [[] for _ in range(len(hashedgraphs))]\n",
    "    for i in range(len(hashedgraphs)):\n",
    "        #get node labels, edgelabels\n",
    "        v = nx.get_node_attributes(hashedgraphs[i], 'labels').values()\n",
    "        e = nx.get_edge_attributes(hashedgraphs[i], 'labels').values()\n",
    "        #extract from list-of-list\n",
    "        v = list(chain.from_iterable(v))\n",
    "        e = list(chain.from_iterable(e))\n",
    "        #add []+a\n",
    "        lv[i] = lv[i] + v\n",
    "        el[i] = el[i] + e\n",
    "\n",
    "    print(f'Hops = {h}')\n",
    "    #For each hop\n",
    "    for _ in range(h):\n",
    "        #For each graph\n",
    "        for i in tqdm(range(len(hashedgraphs))):\n",
    "            #graph to be worked with\n",
    "            Gi = hashedgraphs[i].copy()\n",
    "            #simplify: get biggest connected subgraph\n",
    "            Gi = Gi.subgraph(sorted(nx.connected_components(Gi), key=len, reverse=True)[0])\n",
    "            \n",
    "            #For each node\n",
    "            #Array with columns: node, label, newlabel\n",
    "            lblarri_v = getnodelblarr(Gi)\n",
    "            lblarri_e = getedgelabelarr(Gi)\n",
    "            #Update big alphabet, hash\n",
    "            Alphabet, lblarri_v, currentmax = hashtodic(Alphabet, lblarri_v, currentmax, node=True)\n",
    "            #sort and reset index\n",
    "            lblarri_v = lblarri_v.sort_values(by='node').reset_index(drop=True)  \n",
    "            #relabel to a different graph\n",
    "            # print(lblarri_v, lblarri_e, sep='\\n\\n')\n",
    "            Gi = assignewlabels_node(Gi, lblarri_v)\n",
    "\n",
    "            if len(lblarri_e)!=0:\n",
    "                Alphabet, lblarri_e, currentmax = hashtodic(Alphabet, lblarri_e, currentmax, node=False)\n",
    "                lblarri_e = lblarri_e.sort_values(by='edge').reset_index(drop=True)\n",
    "                Gi = assignewlabels_edge(Gi, lblarri_e)\n",
    "                e = nx.get_edge_attributes(hashedgraphs[i], 'labels').values()\n",
    "                e = list(chain.from_iterable(e))\n",
    "                el[i] = el[i] + e\n",
    "\n",
    "            #assign graph-value\n",
    "            hashedgraphs[i] = Gi\n",
    "            #add counts of labels, reuse previous variable\n",
    "            v = nx.get_node_attributes(hashedgraphs[i], 'labels').values()\n",
    "            v = list(chain.from_iterable(v))\n",
    "            lv[i] = lv[i] + v\n",
    "            \n",
    "    return hashedgraphs, Alphabet, lv, el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashedgraphs, Alphabet, lv, ev = WL(train_graphs + test_graphs, h=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFeatureVectors(hashedgraphs, Alphabet, l):\n",
    "    '''Creating matrix of feature vectors for each graph'''\n",
    "    M = np.zeros((len(hashedgraphs), len(Alphabet)+100))\n",
    "    for i in tqdm(range(len(hashedgraphs))):\n",
    "        a = pd.value_counts(l[i])\n",
    "        M[i, list(a.index)] = a.values\n",
    "\n",
    "    sM = sparse.csr_matrix(M)\n",
    "    #And save so this only needs to be saved once.\n",
    "    return sM\n",
    "\n",
    "sM = generateFeatureVectors(hashedgraphs, Alphabet, lv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Data for easier reloading later in sparse format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import save_sparse_csr, load_sparse_csr\n",
    "save_sparse_csr('Data/WL_allfeatures_e.npz', sM)\n",
    "save_sparse_csr('Data/WL_trainfeatvec_e.npz', sM[0:len(train_graphs)])\n",
    "save_sparse_csr('Data/WL_testfeatvec_e.npz', sM[len(train_graphs):len(train_graphs+test_graphs)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create kernel matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernelFromFeatureVectors(filename, sparse=True):\n",
    "    sM = load_sparse_csr(filename) if sparse else np.loadtxt(filename) #'Data/WLfeaturevectors.npz'\n",
    "    print(f'sM.shape = {sM.shape}')\n",
    "    M = ss.csr_matrix.toarray(sM)\n",
    "    K = np.dot(M, M.T)\n",
    "    sK = ss.csr_matrix(K)\n",
    "    return sK, M\n",
    "\n",
    "sK, M = kernelFromFeatureVectors('Data/WL_allfeatures_e.npz', sparse=True)   \n",
    "print('Kernel Matrix Determined')\n",
    "save_sparse_csr('Data/WLKernel_traintest_e', sK)\n",
    "\n",
    "# Compare how much sparse arrays save space\n",
    "# from sys import getsizeof\n",
    "# M = ss.csr_matrix.toarray(sM)\n",
    "# print(M.shape, getsizeof(M), getsizeof(sM), sep='\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "with open('Data/training_labels.pkl', 'rb') as file:\n",
    "    labels = pkl.load(file)\n",
    "from lib import load_sparse_csr\n",
    "# WLData = load_sparse_csr('Data/WLKernel_traintest_e.npz')\n",
    "# WLData = load_sparse_csr('Data\\WL\\WLKernel_traintest_e_h5.npz')\n",
    "# WLData = ss.csr_matrix.toarray(WLData)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Center dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = len(WLData)\n",
    "# K = WLData\n",
    "# U = (1/N) * np.ones((N,N))\n",
    "# I = np.eye(N)\n",
    "# Kc = (I-U) @ K @ (I-U)\n",
    "# np.savetxt('Data/Kc8000.txt', Kc)\n",
    "Kc = np.loadtxt('Data/Kc8000.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project entire dataset (train-validate-test) to PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# evals, evecs = np.linalg.eigh(Kc)\n",
    "# numpca = 8000\n",
    "# WLDataNew = Kc.dot(evecs[0:numpca].T)\n",
    "# np.savetxt('Kc8000_pcaed.txt', WLDataNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "with open('Data/training_labels.pkl', 'rb') as file:\n",
    "    labels = pkl.load(file)\n",
    "\n",
    "WLDataNew = np.loadtxt('Kc8000_pcaed.txt')\n",
    "N = 6000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Linear Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2000) (400, 2000)\n",
      "AUC: 0.7722304102186195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\norti\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Choose subset\n",
    "t = N//3\n",
    "r = N//3\n",
    "WLTrainValid = WLDataNew[0:t, 0:r] #, 0:len(train_graphs)]\n",
    "WLLabels = labels[0:t]\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(WLTrainValid, WLLabels, test_size=0.2) # random_state=1)\n",
    "\n",
    "\n",
    "print(X_train.shape, X_validate.shape)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "lr = LogisticRegression(solver='liblinear', verbose=False, penalty='l2', C=200, warm_start=False)\n",
    "# lr = SVC(C=10, kernel='linear')\n",
    "# Fit on training data\n",
    "lr.fit(X_train, y_train)\n",
    "#Predict on validation data\n",
    "pred = lr.predict(X_validate)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_validate, pred, pos_label=1)\n",
    "#AUC metric\n",
    "print(f'AUC: {metrics.auc(fpr, tpr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import LIN, KernelSVC\n",
    "\n",
    "sigma = 1.5\n",
    "C=100.\n",
    "kernel = LIN().kernel\n",
    "linmodel = KernelSVC(C=C, kernel=kernel)\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(WLTrainValid, WLLabels, test_size=0.2) # random_state=1)\n",
    "\n",
    "linmodel.fit(X_train, y_train)\n",
    "linmodel.predict(X_validate)\n",
    "# plotClassification(train_dataset['x'], train_dataset['y'], model, label='Training')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range:\n",
      " [-258.73844563838435, 36.04365338911715]\n"
     ]
    }
   ],
   "source": [
    "from lib import calculateLogits\n",
    "X_test = WLDataNew[6000:8000, 0:r]\n",
    "pred_test = lr.predict_proba(X_test)\n",
    "logit = calculateLogits(pred_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import saveDataToFormattedSubmissionFile\n",
    "ctr = int(input('Current submission attempt:\\t'))\n",
    "saveDataToFormattedSubmissionFile(logit, f'Data/WL_test_pred{ctr}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'Data/Models/lr795.sav'\n",
    "pkl.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to relevant file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0392687b3784acca957340e081afceb5f7dd41424de88c7591eaa73ceb6efaf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
