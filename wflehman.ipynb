{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from scipy import sparse\n",
    "import scipy.sparse as ss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def SortLabels(GF, nodeindex, labeldict):\n",
    "#     '''Get all neighbours of a node at nodeindex and sort them'''\n",
    "#     neigh = list(GF.neighbors(nodeindex))\n",
    "#     label = [labeldict[i] for i in neigh]\n",
    "#     label = list(chain.from_iterable(label))\n",
    "#     label.sort()\n",
    "#     return label\n",
    "\n",
    "# def GlueLabels(nodelabeldict, index, newlist):\n",
    "#     '''Glue together a list of nodelabels with the node's own label'''\n",
    "#     nodelbl = nodelabeldict[index][0]\n",
    "#     x = ''.join([str(s) for s in newlist])\n",
    "#     return str(nodelbl) + x\n",
    "\n",
    "# def dictToArray(dict):\n",
    "#     '''Helper function'''\n",
    "#     a = np.array(list(dict.items()), dtype=object )\n",
    "#     a[:,1] = list(chain.from_iterable(a[:,1]))\n",
    "#     return a\n",
    "\n",
    "# def draw(G):\n",
    "#     '''Draw graph with its labels'''\n",
    "#     pos = nx.spring_layout(G, seed=1, k=0.3)\n",
    "#     nx.draw(G, pos)\n",
    "#     node_labels = nx.get_node_attributes(G, \"labels\")\n",
    "#     nx.draw_networkx_labels(G, pos, node_labels, font_size=8);\n",
    "\n",
    "# def getnodelblarr(GF):\n",
    "#     '''For a specific graph, generate a (sorted) dataframe of old labels and new glued labels for each node'''\n",
    "#     #Dictionary of labels for graph\n",
    "#     NLbl_dict = nx.get_node_attributes(GF, 'labels')\n",
    "#     #Generate new lbls for each vertex\n",
    "#     newlabels = []\n",
    "#     for v in NLbl_dict:\n",
    "#         sortedlbls = SortLabels(GF, v, NLbl_dict)\n",
    "#         gluedlbl = GlueLabels(NLbl_dict, v, sortedlbls)\n",
    "#         newlabels.append(gluedlbl)\n",
    "\n",
    "#     #Array with columns: node, label, newlabel\n",
    "#     Nlbl_arra0 = np.c_[dictToArray(NLbl_dict), newlabels]\n",
    "#     #And sort\n",
    "#     Nlbl_arra0 = Nlbl_arra0[Nlbl_arra0[:,2].argsort()]\n",
    "#     return Nlbl_arra0\n",
    "\n",
    "# def hashtodic(ALPHAbet, newlblarr, currentmax):\n",
    "#     '''Function to hash newly glued labels; then add the (unique) new ones to the overall alphabet.\n",
    "#     Return a Dataframe with new, old, hashed labels for each node'''\n",
    "#     #Get unique entries\n",
    "#     a = np.unique(newlblarr[:,2]) \n",
    "#     #Get those that are new also\n",
    "#     a = [a[i] for i in range(len(a)) if a[i] not in ALPHAbet]\n",
    "#     #Hash values\n",
    "#     b = np.arange(len(a))+currentmax\n",
    "#     currentmax = currentmax+len(b)\n",
    "#     dic1 = ALPHAbet | {a[i] : b[i] for i in range(len(a))}\n",
    "#     #relabel\n",
    "#     newlblarr = np.c_[newlblarr, [dic1[newlblarr[:,2][_]] for _ in range(len(newlblarr))]]    \n",
    "    \n",
    "#     return dic1, pd.DataFrame(newlblarr, columns=[\"node\", 'oldlbl', 'hashed', 'newlbl']), currentmax\n",
    " \n",
    "# def assignewlabels(GF, nlblarr):\n",
    "#     '''Helper function: change a graph's labels. (Change directly as working on a copy) '''\n",
    "#     newdict = {nlblarr['node'][i] : [nlblarr['newlbl'][i]] for i in range(len(nlblarr))}\n",
    "#     H = GF.copy()\n",
    "#     nx.set_node_attributes(H, newdict, name='labels')\n",
    "#     return H\n",
    "\n",
    "\n",
    "# def save_sparse_csr(filename, array):\n",
    "#     np.savez(filename, data=array.data, indices=array.indices,\n",
    "#              indptr=array.indptr, shape=array.shape)\n",
    "\n",
    "# def load_sparse_csr(filename):\n",
    "#     loader = np.load(filename)\n",
    "#     return sparse.csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "#                       shape=loader['shape'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import getnodelblarr, hashtodic, assignewlabels\n",
    "def WL(graphs, h=4):\n",
    "    #Number of hops\n",
    "    h = 4\n",
    "    N = len(graphs)# N = len(train_graphs)//1\n",
    "    \n",
    "    hashedgraphs = graphs#train_graphs.copy()\n",
    "    hashedgraphs = hashedgraphs[0:N]\n",
    "    Alphabet = {}#{f'{i}': i for i in range(49)} #{}\n",
    "    currentmax = 49\n",
    "\n",
    "    #Alphabet instances for each graph over all hops\n",
    "    #List of N empty lists\n",
    "    l = [[] for _ in range(len(hashedgraphs))]\n",
    "    for i in range(len(hashedgraphs)):\n",
    "        a = nx.get_node_attributes(hashedgraphs[i], 'labels').values()\n",
    "        a = list(chain.from_iterable(a))\n",
    "        l[i] = l[i] + a\n",
    "    pd.value_counts(l[0])\n",
    "\n",
    "    print(f'Hops = {h}')\n",
    "    maxes = [currentmax]\n",
    "    #For each hop\n",
    "    for _ in range(h):\n",
    "        #For each graph\n",
    "        for i in tqdm(range(len(hashedgraphs))):\n",
    "            Gi = hashedgraphs[i].copy()# Gi = train_graphs[i]\n",
    "            #get biggest connected subgraph\n",
    "            Gi = Gi.subgraph(sorted(nx.connected_components(Gi), key=len, reverse=True)[0])\n",
    "            #For each node\n",
    "            #Array with columns: node, label, newlabel\n",
    "            lblarri = getnodelblarr(Gi)\n",
    "            #Update big alphabet, hash\n",
    "            Alphabet, lblarri, currentmax = hashtodic(Alphabet, lblarri, currentmax=currentmax)\n",
    "            #sort and reset index\n",
    "            lblarri = lblarri.sort_values(by='node').reset_index(drop=True)\n",
    "            # lblarri = lblarri\n",
    "            #relabel to a different graph\n",
    "            Gi = assignewlabels(Gi, lblarri)\n",
    "            #assign graph-value\n",
    "            hashedgraphs[i] = Gi\n",
    "            #add counts of labels\n",
    "            a = nx.get_node_attributes(hashedgraphs[i], 'labels').values()\n",
    "            a = list(chain.from_iterable(a))\n",
    "            l[i] = l[i] + a\n",
    "    return hashedgraphs, Alphabet, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hops = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:12<00:00, 652.83it/s]\n",
      "100%|██████████| 8000/8000 [00:13<00:00, 574.36it/s]\n",
      "100%|██████████| 8000/8000 [00:21<00:00, 379.66it/s]\n",
      "100%|██████████| 8000/8000 [00:52<00:00, 153.78it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('Data/training_data.pkl', 'rb') as file:\n",
    "    train_graphs = pkl.load(file)\n",
    "with open('Data/test_data.pkl', 'rb') as file:\n",
    "    test_graphs = pkl.load(file)\n",
    "    \n",
    "hashedgraphs, Alphabet, l = WL(train_graphs+test_graphs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:02<00:00, 3909.40it/s]\n"
     ]
    }
   ],
   "source": [
    "def generateFeatureVectors(hashedgraphs, Alphabet, l):\n",
    "    '''Creating matrix of feature vectors for each graph'''\n",
    "    M = np.zeros((len(hashedgraphs), len(Alphabet)+100))\n",
    "    for i in tqdm(range(len(hashedgraphs))):\n",
    "        a = pd.value_counts(l[i])\n",
    "        M[i, list(a.index)] = a.values\n",
    "\n",
    "    sM = sparse.csr_matrix(M)\n",
    "    #And save so this only needs to be saved once.\n",
    "    return sM\n",
    "\n",
    "sM = generateFeatureVectors(hashedgraphs, Alphabet, l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Data for easier reloading later in sparse format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import save_sparse_csr, load_sparse_csr\n",
    "save_sparse_csr('Data/WL_allfeatures.npz', sM)\n",
    "save_sparse_csr('Data/WL_trainfeatvec.npz', sM[0:len(train_graphs)])\n",
    "save_sparse_csr('Data/WL_testfeatvec.npz', sM[len(train_graphs):len(train_graphs+test_graphs)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create kernel matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sM.shape = (8000, 69296)\n",
      "Kernel Matrix Determined\n"
     ]
    }
   ],
   "source": [
    "def kernelFromFeatureVectors(filename, sparse=True):\n",
    "    sM = load_sparse_csr(filename) if sparse else np.loadtxt(filename) #'Data/WLfeaturevectors.npz'\n",
    "    print(f'sM.shape = {sM.shape}')\n",
    "    M = ss.csr_matrix.toarray(sM)\n",
    "    K = np.dot(M, M.T)\n",
    "    sK = ss.csr_matrix(K)\n",
    "    return sK, M\n",
    "\n",
    "sK, M = kernelFromFeatureVectors('Data/WL_allfeatures.npz', sparse=True)   \n",
    "print('Kernel Matrix Determined')\n",
    "save_sparse_csr('Data/WLKernel_traintest', sK)\n",
    "\n",
    "# Compare how much sparse arrays save space\n",
    "# from sys import getsizeof\n",
    "# M = ss.csr_matrix.toarray(sM)\n",
    "# print(M.shape, getsizeof(M), getsizeof(sM), sep='\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "with open('Data/training_labels.pkl', 'rb') as file:\n",
    "    labels = pkl.load(file)\n",
    "from lib import load_sparse_csr\n",
    "WLData = load_sparse_csr('Data/WLKernel_traintest.npz')\n",
    "WLData = ss.csr_matrix.toarray(WLData)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project entire dataset (train-validate-test) to PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "evals, evecs = np.linalg.eigh(WLData)\n",
    "numpca = 6000\n",
    "WLDataNew = WLData.dot(evecs[0:numpca].T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WLTrainValid = WLDataNew[0:len(train_graphs), 0:len(train_graphs)]\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(WLTrainValid, labels, test_size=0.3, random_state=1)\n",
    "X_test = WLDataNew[len(train_graphs): len(train_graphs+test_graphs)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Linear Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "lr = LogisticRegression(random_state=0, solver='newton-cholesky')\n",
    "# Fit on training data\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on validation data\n",
    "pred = lr.predict(X_validate)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_validate, pred, pos_label=1)\n",
    "#AUC metric\n",
    "print(f'AUC: {metrics.auc(fpr, tpr)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import calculateLogits\n",
    "pred_test = lr.predict_proba(X_test)\n",
    "logit = calculateLogits(pred_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to relevant file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import saveDataToFormattedSubmissionFile\n",
    "ctr = int(input('Current submission attempt:\\t'))\n",
    "saveDataToFormattedSubmissionFile(logit, f'Data/WL_test_pred{ctr}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0392687b3784acca957340e081afceb5f7dd41424de88c7591eaa73ceb6efaf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
