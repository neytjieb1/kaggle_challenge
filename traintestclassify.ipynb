{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split our data\n",
    "\n",
    "_My suggestion would be to use a split with these parameters for our validation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "with open('training_data.pkl', 'rb') as file:\n",
    "    train_graphs = pkl.load(file)\n",
    "with open('training_labels.pkl', 'rb') as file:\n",
    "    train_labels = pkl.load(file)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(train_graphs, train_labels, test_size=0.1, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sklearn multiple classifiers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code source: Gaël Varoquaux\n",
    "# #              Andreas Müller\n",
    "# # Modified for documentation by Jaques Grobler\n",
    "# # License: BSD 3 clause\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "# from sklearn.gaussian_process.kernels import RBF\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# from sklearn.inspection import DecisionBoundaryDisplay\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025, probability=True),\n",
    "    SVC(gamma=2, C=1, probability=True),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "def myplot(ax, X_train, X_test, y_train, y_test, x_min, x_max, y_min, y_max, title):\n",
    "    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "    # testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"  )\n",
    "    # set axes\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
    ")\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable_ds = (X, y)\n",
    "\n",
    "datasets = [\n",
    "    make_moons(noise=0.3, random_state=0),\n",
    "    make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "    linearly_separable_ds,\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(27, 9))\n",
    "\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess; split into training/test part\n",
    "    X, y = ds\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42 )\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "    # plot dataset\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    title = \"Input data\" if ds_cnt == 0 else \"\"\n",
    "    myplot(ax, X_train, X_test, y_train, y_test, x_min, x_max, y_min, y_max, title)\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "\n",
    "        #Fit data\n",
    "        clf = make_pipeline(StandardScaler(), clf)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        # pred = clf.predict_proba(X_test)\n",
    "        # print(metrics.f1_score(y_test, clf.predict(X_test)))\n",
    "        \n",
    "        cm = plt.cm.RdBu\n",
    "        DecisionBoundaryDisplay.from_estimator(\n",
    "            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "        )\n",
    "\n",
    "        # Plot the training points\n",
    "        title = name if ds_cnt == 0 else \"\"\n",
    "        myplot(ax, X_train, X_test, y_train, y_test, x_min, x_max, y_min, y_max, title)\n",
    "        ax.text(x_max - 0.3, y_min + 0.3,  (\"%.2f\" % score).lstrip(\"0\"),  size=15,  horizontalalignment=\"right\",  )\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle\n",
    "\n",
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 6000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Data/training_labels.pkl', 'rb') as file:\n",
    "    labels = pkl.load(file)\n",
    "from lib import load_sparse_csr\n",
    "WLData = load_sparse_csr('Data/WLKernel.npz')\n",
    "WLData = ss.csr_matrix.toarray(WLData)\n",
    "WLData.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single classifier: KNN, Cross-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossfold(classif, X, y, n_splits, classif):\n",
    "    n_splits = 10\n",
    "    kf = KFold(n_splits=n_splits, random_state=1, shuffle=True)\n",
    "    kf.get_n_splits(X)\n",
    "    scores = np.empty(n_splits)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        #Split\n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        #Fit\n",
    "        clf = make_pipeline(StandardScaler(), classif)\n",
    "        clf.fit(X_train, y_train)\n",
    "        #Hit\n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "        scores[i] = clf.score(X_test, y_test)\n",
    "        pred = clf.predict(X_test)\n",
    "        print(f\"Fold {i}: {scores[i]}, {metrics.f1_score(y_test, pred)}\")\n",
    "        # print(f'{scores[i]*len(y_test)}/{len(y_test)}')\n",
    "        # print(f'')\n",
    "        #Count number of 1's \n",
    "        # pred[y_test==1]\n",
    "    \n",
    "    return scores, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over classifiers\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(WLData, labels, test_size=0.3, random_state=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a classifier instance\n",
    "#\n",
    "# rf = RandomForestClassifier(n_estimators=500, min_samples_leaf=1, max_depth=50)\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "rbf = SVC(gamma=2, C=1, probability=True, verbose=1)\n",
    "rbf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf.fit(X_train, y_train)\n",
    "pred = rbf.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35365853658536583\n",
      "0.93\n",
      "0.4793388429752066\n"
     ]
    }
   ],
   "source": [
    "print(metrics.recall_score(y_validate, pred))\n",
    "print(metrics.accuracy_score(y_validate, pred))\n",
    "print(metrics.f1_score(y_validate, pred))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(y_validate, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 6000)\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import scipy.sparse as ss\n",
    "from sklearn.model_selection import train_test_split\n",
    "with open('Data/training_labels.pkl', 'rb') as file:\n",
    "    labels = pkl.load(file)\n",
    "from lib import load_sparse_csr\n",
    "WLData = load_sparse_csr('Data/WLKernel.npz')\n",
    "WLData = ss.csr_matrix.toarray(WLData)\n",
    "print(WLData.shape)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(WLData, labels, test_size=0.3, random_state=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok restart tonight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "evals, evecs = np.linalg.eigh(WLData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpca = 4000\n",
    "WLDataNew = WLData.dot(evecs[0:numpca].T)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(WLDataNew, labels, test_size=0.3, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-19 {color: black;background-color: white;}#sk-container-id-19 pre{padding: 0;}#sk-container-id-19 div.sk-toggleable {background-color: white;}#sk-container-id-19 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-19 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-19 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-19 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-19 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-19 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-19 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-19 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-19 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-19 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-19 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-19 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-19 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-19 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-19 div.sk-item {position: relative;z-index: 1;}#sk-container-id-19 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-19 div.sk-item::before, #sk-container-id-19 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-19 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-19 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-19 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-19 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-19 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-19 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-19 div.sk-label-container {text-align: center;}#sk-container-id-19 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-19 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-19\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, gamma=0.5, probability=True, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" checked><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, gamma=0.5, probability=True, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1, gamma=0.5, probability=True, verbose=1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "rbf = SVC(gamma=0.5, C=1, probability=True, verbose=1)\n",
    "rbf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6082279802015624\n"
     ]
    }
   ],
   "source": [
    "# X_validate_pca = pca.transform(X_validate)\n",
    "pred = rbf.predict(X_validate)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_validate, pred, pos_label=1)\n",
    "print(f'AUC: {metrics.auc(fpr, tpr)}')\n",
    "# AUC: 0.6082279802015624 for gamma = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logproba = rbf.predict_proba(X_validate)\n",
    "# [-0.0063112232179776875, -5.068579719299193]\n",
    "logit = np.log(logproba[:,1]/(1-logproba[:,1])) \n",
    "[np.min(logit), np.max(logit)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=1, max_iter=2000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=1, max_iter=2000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=1, max_iter=2000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(alpha=1, max_iter=2000)\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "pred = mlp.predict(X_validate)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_validate, pred, pos_label=1)\n",
    "print(f'AUC: {metrics.auc(fpr, tpr)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6505978293279265\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_depth=500, n_estimators=100, max_features=10)\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_validate)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_validate, pred, pos_label=1)\n",
    "print(f'AUC: {metrics.auc(fpr, tpr)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7639617747033215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=0, solver='newton-cholesky')\n",
    "lr.fit(X_train, y_train)\n",
    "pred = lr.predict(X_validate)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_validate, pred, pos_label=1)\n",
    "print(f'AUC: {metrics.auc(fpr, tpr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-372.34819604074823, 33.33560318801494]\n"
     ]
    }
   ],
   "source": [
    "def calculateLogits(pred):\n",
    "    logproba = pred\n",
    "    logproba[list(np.where(logproba[:,1]==1)[0]), 1] = 1-1e-10\n",
    "    logit = np.log(logproba[:,1]/(1-logproba[:,1])) \n",
    "    print([np.min(logit), np.max(logit)])\n",
    "    return logit\n",
    "\n",
    "predValidate = lr.predict_proba(X_validate)\n",
    "logit = calculateLogits(predValidate)# logproba = lr.predict_proba(X_validate)\n",
    "\n",
    "\n",
    "with open('Data/test_data.pkl', 'rb') as file:\n",
    "    train_graphs = pkl.load(file)\n",
    "predTest = lr.predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test data \n",
    "# for instance, \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DP Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "DPdata = np.loadtxt('Data/DPKernel.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "with open(\"Data/training_data.pkl\", 'rb') as file:\n",
    "    train_graphs = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_product(G1, G2, node_label='labels', edge_label='labels'):\n",
    "    from itertools import product\n",
    "    GP = nx.Graph()\n",
    "    # add nodes\n",
    "    for u, v in product(G1, G2):\n",
    "        if G1.nodes[u][node_label] == G2.nodes[v][node_label]:\n",
    "            GP.add_node((u, v))\n",
    "            GP.nodes[(u, v)].update({node_label: G1.nodes[u][node_label]})\n",
    "\n",
    "    # add edges\n",
    "    for u, v in product(GP, GP):\n",
    "            if (u[0], v[0]) in G1.edges and (u[1], v[1]) in G2.edges and G1.edges[u[0],v[0]][edge_label] == G2.edges[u[1],v[1]][edge_label]:\n",
    "                GP.add_edge((u[0], u[1]), (v[0], v[1]))\n",
    "                GP.edges[(u[0], u[1]), (v[0], v[1])].update({edge_label:G1.edges[u[0], v[0]][edge_label]})\n",
    "\n",
    "    return GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as ss\n",
    "def DPKernel(Gx):\n",
    "    adjacencymatrix = nx.adjacency_matrix(Gx)\n",
    "    rho = ss.linalg.norm(adjacencymatrix)\n",
    "    infsum = ss.linalg.inv( ss.eye(adjacencymatrix.shape[0], format='csc') - rho*adjacencymatrix )\n",
    "    k = np.sum(np.abs(infsum))\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 311/6000 [3:46:05<68:55:43, 43.62s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14436\\2286334340.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mG2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_graphs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mGx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirect_product\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mK_dp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDPKernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m#extract diagonal and then make into diagonal matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mK_dp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK_dp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mK_dp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK_dp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\norti\\OneDrive - Université Paris-Dauphine\\Documents\\GitHub\\kaggle_challenge\\kaggle_challenge\\lib.py\u001b[0m in \u001b[0;36mDPKernel\u001b[1;34m(DPG)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[0madjacencymatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjacency_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDPG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[0mrho\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madjacencymatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[0minfsum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madjacencymatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrho\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0madjacencymatrix\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfsum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\norti\\anaconda3\\lib\\site-packages\\scipy\\sparse\\linalg\\_matfuncs.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(A)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;31m# Use sparse direct solver to solve \"AX = I\" accurately\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ident_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0mAinv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspsolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mAinv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\norti\\anaconda3\\lib\\site-packages\\scipy\\sparse\\linalg\\_dsolve\\linsolve.py\u001b[0m in \u001b[0;36mspsolve\u001b[1;34m(A, b, permc_spec, use_umfpack)\u001b[0m\n\u001b[0;32m    252\u001b[0m                 \u001b[1;31m# That is a slightly faster code path.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mbj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                 \u001b[0mxj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAfactsolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m                 \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 \u001b[0msegment_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lib import direct_product , DPKernel\n",
    "\n",
    "N = len(train_graphs)\n",
    "K_dp = np.zeros((N,N))\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    G1 = train_graphs[i]\n",
    "    for j in range(i+1): #K will be symmetric, so only calculate upper and then change\n",
    "        G2 = train_graphs[j]        \n",
    "        Gx = direct_product(G1, G2)\n",
    "        K_dp[i,j] = DPKernel(Gx)\n",
    "#extract diagonal and then make into diagonal matrix\n",
    "K_dp = K_dp + K_dp.T - np.diag(np.diag(K_dp)) \n",
    "\n",
    "np.savetxt('Data/DPKernel.txt', K_dp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0392687b3784acca957340e081afceb5f7dd41424de88c7591eaa73ceb6efaf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
